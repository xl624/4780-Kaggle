{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from collections import Counter\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import datetime\n",
    "from vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "import re\n",
    "from nltk.tag import StanfordPOSTagger, StanfordNERTagger\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average num char when y = 1: ', 123.62358642972536)\n",
      "('Average num char when y = -1: ', 103.26382978723404)\n",
      "('Average num words when y = 1: ', 20.248788368336026)\n",
      "('Average num words when y = -1: ', 13.476595744680852)\n",
      "('Average word length when y = 1: ', 5.420011027379853)\n",
      "('Average word length when y = -1: ', 8.05688745871895)\n",
      "('Average num uppercase word when y = 1: ', 0.5977382875605816)\n",
      "('Average num uppercase word when y = -1: ', 0.44042553191489364)\n",
      "('Percent whole sentence upper when y = 1: ', 0.01938610662358643)\n",
      "('Percent whole sentence upper when y = -1: ', 0.09148936170212765)\n",
      "('Average num url when y = 1: ', 0.0888529886914378)\n",
      "('Average num url when y = -1: ', 0.8957446808510638)\n",
      "('Average num mention when y = 1: ', 0.9612277867528272)\n",
      "('Average num mention when y = -1: ', 0.2680851063829787)\n",
      "('Average num hashtag when y = 1: ', 0.13408723747980614)\n",
      "('Average num hashtag when y = -1: ', 0.9595744680851064)\n",
      "('Average num number when y = 1: ', 0.18739903069466882)\n",
      "('Average num number when y = -1: ', 0.21914893617021278)\n",
      "('Average num smiley when y = 1: ', 0.022617124394184167)\n",
      "('Average num smiley when y = -1: ', 0.010638297872340425)\n",
      "('Average num start with \" when y = 1: ', 0.3182552504038772)\n",
      "('Average num start with \" when y = -1: ', 0.006382978723404255)\n",
      "('Average num start with @ when y = 1: ', 0.004846526655896607)\n",
      "('Average num start with @ when y = -1: ', 0.0)\n",
      "('Average num start with # when y = 1: ', 0.0)\n",
      "('Average num start with # when y = -1: ', 0.059574468085106386)\n",
      "('Average num ; when y = 1: ', 0.7512116316639742)\n",
      "('Average num ; when y = -1: ', 0.5617021276595745)\n",
      "('Average num ! when y = 1: ', 0.7996768982229402)\n",
      "('Average num ! when y = -1: ', 0.8723404255319149)\n",
      "('Average num \" when y = 1: ', 0.7140549273021002)\n",
      "('Average num \" when y = -1: ', 0.0425531914893617)\n",
      "('Average num ... when y = 1: ', 0.011308562197092083)\n",
      "('Average num ... when y = -1: ', 0.00425531914893617)\n",
      "('Average favorite when y = 1: ', 15923.681744749596)\n",
      "('Average favorite when y = -1: ', 15381.478723404256)\n",
      "('Average retweet when y = 1: ', 5269.670436187399)\n",
      "('Average retweet when y = -1: ', 5630.444680851064)\n",
      "('Average retweet when y = 1: ', 541.2342487883683)\n",
      "('Average retweet when y = -1: ', 547.6425531914894)\n"
     ]
    }
   ],
   "source": [
    "p.set_options(p.OPT.URL, p.OPT.EMOJI,p.OPT.MENTION,p.OPT.HASHTAG,p.OPT.RESERVED,p.OPT.SMILEY,p.OPT.NUMBER)\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "data['tokenized'] = data['text'].apply(p.tokenize)\n",
    "print(\"Average num char when y = 1: \", np.mean(data.loc[data['label']==1,'text'].apply(len)))\n",
    "print(\"Average num char when y = -1: \", np.mean(data.loc[data['label']==-1,'text'].apply(len)))\n",
    "print(\"Average num words when y = 1: \", np.mean(data.loc[data['label']==1,'text'].apply(lambda x: len(x.split()))))\n",
    "print(\"Average num words when y = -1: \", np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: len(x.split()))))\n",
    "print(\"Average word length when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'text'].apply(lambda x: np.mean([len(a) for a in x.split()]))))\n",
    "print(\"Average word length when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: np.mean([len(a) for a in x.split()]))))\n",
    "print(\"Average num uppercase word when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'text'].apply(lambda x: len(re.findall('\\s([A-Z][A-Z]+)', x)))))\n",
    "print(\"Average num uppercase word when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: len(re.findall('\\s([A-Z][A-Z]+)', x)))))\n",
    "print(\"Percent whole sentence upper when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'tokenized'].apply(lambda x: x.isupper())))\n",
    "print(\"Percent whole sentence upper when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'tokenized'].apply(lambda x: x.isupper())))\n",
    "print(\"Average num url when y = 1: \", np.mean(data.loc[data['label']==1,'tokenized'].apply(lambda x: x.count('$URL$'))))\n",
    "print(\"Average num url when y = -1: \", np.mean(data.loc[data['label']==-1,'tokenized'].apply(lambda x: x.count('$URL$'))))\n",
    "print(\"Average num mention when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'tokenized'].apply(lambda x: x.count('$MENTION$'))))\n",
    "print(\"Average num mention when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'tokenized'].apply(lambda x: x.count('$MENTION$'))))\n",
    "print(\"Average num hashtag when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'tokenized'].apply(lambda x: x.count('$HASHTAG$'))))\n",
    "print(\"Average num hashtag when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'tokenized'].apply(lambda x: x.count('$HASHTAG$'))))\n",
    "print(\"Average num number when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'tokenized'].apply(lambda x: x.count('$NUMBER$'))))\n",
    "print(\"Average num number when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'tokenized'].apply(lambda x: x.count('$NUMBER$'))))\n",
    "print(\"Average num smiley when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'tokenized'].apply(lambda x: x.count('$SMILEY$'))))\n",
    "print(\"Average num smiley when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'tokenized'].apply(lambda x: x.count('$SMILEY$'))))\n",
    "print(\"Average num start with \\\" when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'tokenized'].apply(lambda x: len(re.findall('^\"', x)))))\n",
    "print(\"Average num start with \\\" when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'tokenized'].apply(lambda x: len(re.findall('^\"', x)))))\n",
    "print(\"Average num start with @ when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'text'].apply(lambda x: len(re.findall('^@', x)))))\n",
    "print(\"Average num start with @ when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: len(re.findall('^@', x)))))\n",
    "print(\"Average num start with # when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'text'].apply(lambda x: len(re.findall('^#', x)))))\n",
    "print(\"Average num start with # when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: len(re.findall('^#', x)))))\n",
    "print(\"Average num ; when y = 1: \", np.mean(data.loc[data['label']==1,'text'].apply(lambda x: x.count(\";\"))))\n",
    "print(\"Average num ; when y = -1: \", np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: x.count(\";\"))))\n",
    "print(\"Average num ! when y = 1: \", np.mean(data.loc[data['label']==1,'text'].apply(lambda x: x.count(\"!\"))))\n",
    "print(\"Average num ! when y = -1: \", np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: x.count(\"!\"))))\n",
    "print(\"Average num \\\" when y = 1: \", np.mean(data.loc[data['label']==1,'text'].apply(lambda x: x.count(\"\\\"\"))))\n",
    "print(\"Average num \\\" when y = -1: \", np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: x.count(\"\\\"\"))))\n",
    "print(\"Average num ... when y = 1: \", np.mean(data.loc[data['label']==1,'text'].apply(lambda x: x.count(\"...\"))))\n",
    "print(\"Average num ... when y = -1: \", np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: x.count(\"...\"))))\n",
    "print(\"Average favorite when y = 1: \", np.mean(data.loc[data['label']==1,'favoriteCount']))\n",
    "print(\"Average favorite when y = -1: \", np.mean(data.loc[data['label']==-1,'favoriteCount']))\n",
    "print(\"Average retweet when y = 1: \", np.mean(data.loc[data['label']==1,'retweetCount']))\n",
    "print(\"Average retweet when y = -1: \", np.mean(data.loc[data['label']==-1,'retweetCount']))\n",
    "print(\"Average id when y = 1: \", np.mean(data.loc[data['label']==1,'id']))\n",
    "print(\"Average id when y = -1: \", np.mean(data.loc[data['label']==-1,'id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"test.csv\")\n",
    "data['created']=pd.to_datetime(data['created'], errors='coerce')\n",
    "data['dayInWeek'] = data['created'].dt.weekday\n",
    "data['hourInDay'] = data['created'].dt.hour\n",
    "data['tokenized'] = data['text'].apply(p.tokenize)\n",
    "data['numchar'] = data['text'].apply(len)\n",
    "data['numWords'] = data['text'].apply(lambda x: len(x.split()))\n",
    "data['avgWordLen'] =data['text'].apply(lambda x: np.mean([len(a) for a in x.split()]))\n",
    "data['wholeUpper'] = data['tokenized'].apply(lambda x: np.sum(x.isupper()))\n",
    "data['numURL'] = data['tokenized'].apply(lambda x: x.count('$URL$'))\n",
    "data['numHash'] = data['tokenized'].apply(lambda x: x.count('$HASHTAG$'))\n",
    "data['mention'] = data['tokenized'].apply(lambda x: x.count('$MENTION$'))\n",
    "data['startQuote']=data['tokenized'].apply(lambda x: len(re.findall('^\"', x)))\n",
    "data['startMention']=data['tokenized'].apply(lambda x: len(re.findall('^@', x)))\n",
    "data['startHash']=data['tokenized'].apply(lambda x: len(re.findall('^#', x)))\n",
    "data['numComa']=data['text'].apply(lambda x: x.count(\";\"))\n",
    "data['numQuoSym']=data['text'].apply(lambda x: x.count(\"\\\"\"))\n",
    "data['posTagged']=data['tokenized'].apply(posTagging)\n",
    "# data['created']=pd.to_datetime(data['created'], errors='coerce')\n",
    "# data = period_of_day(data, 'created')\n",
    "# data = time_of_day(data, 'created')\n",
    "# data = day_of_week(data, 'created')\n",
    "# data = weekend(data, 'created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = TfidfVectorizer(ngram_range=(1, 2),max_df=.99,min_df=.01)\n",
    "a = text.fit_transform(data['text'])\n",
    "colnames = text.get_feature_names()\n",
    "a.todense().shape\n",
    "idx = data.index\n",
    "text = pd.DataFrame(a.todense(),columns=[colnames],index=idx)\n",
    "\n",
    "pos = TfidfVectorizer(ngram_range=(2, 3),max_df=.99,min_df=.01)\n",
    "a = pos.fit_transform(data['posTagged'])\n",
    "colnames = pos.get_feature_names()\n",
    "a.todense().shape\n",
    "idx = data.index\n",
    "pos = pd.DataFrame(a.todense(),columns=[colnames],index=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data,text,pos],axis=1)\n",
    "#data = apply_vader(data, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['statusSource'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-214-20b0ee4ae532>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'favorited'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'replyToSID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'truncated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'screenName'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'isRetweet'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'retweeted'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'favoriteCount'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'replyToSN'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'created'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'id.1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'replyToUID'\u001b[0m\u001b[1;33m,\u001b[0m               \u001b[1;34m\"longitude\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'latitude'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'tokenized'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'posTagged'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'statusSource'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3695\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3696\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3697\u001b[1;33m                                            errors=errors)\n\u001b[0m\u001b[0;32m   3698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3699\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3111\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3141\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3143\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3144\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\pandas\\core\\indexes\\base.pyc\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   4402\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4403\u001b[0m                 raise KeyError(\n\u001b[1;32m-> 4404\u001b[1;33m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[0;32m   4405\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4406\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['statusSource'] not found in axis\""
     ]
    }
   ],
   "source": [
    "data=data.drop(['favorited', 'replyToSID', 'truncated','screenName','isRetweet','retweeted'],axis=1)\n",
    "data=data.drop(['id', 'text', 'favoriteCount','replyToSN','created','id.1','replyToUID',\\\n",
    "               \"longitude\",'latitude','tokenized','posTagged'],axis=1)\n",
    "data = data.drop(['statusSource'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['posTagged'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9078341013824884"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "X = data.drop(['label'], axis=1)\n",
    "Y = pd.DataFrame(np.where(data['label'] == 1, 1,0))\n",
    "# X_train = data\n",
    "X = np.squeeze(X.values)\n",
    "Y = Y.values.flatten()\n",
    "idx = np.random.permutation(len(X))\n",
    "cut = int(np.ceil(len(X)*0.8))\n",
    "X_train = X[idx[:cut],]\n",
    "Y_train = Y[idx[:cut]]\n",
    "gb = GradientBoostingClassifier(n_estimators=200,\n",
    "                                        learning_rate=.1,\n",
    "                                        max_depth=6,\n",
    "                                        min_samples_split=2,\n",
    "                                        min_samples_leaf=1,\n",
    "                                        subsample=1,\n",
    "                                        max_features=None\n",
    "                                        ).fit(X_train, Y_train)\n",
    "X_valid = X[idx[cut:],]\n",
    "Y_valid = Y[idx[cut:]]\n",
    "predicted = gb.predict(X_valid)\n",
    "sum(predicted==Y_valid)/float(len(Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(872L, 1813L)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nerTagging(text):\n",
    "    jar = \"C:\\\\Users\\\\lxykh\\\\OneDrive\\Desktop\\\\4780\\\\2018Fall\\\\kaggle\\\\stanford-ner-2018-10-16\\\\stanford-ner.jar\"\n",
    "    cla = \"C:\\\\Users\\\\lxykh\\\\OneDrive\\Desktop\\\\4780\\\\2018Fall\\\\kaggle\\\\stanford-ner-2018-10-16\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz\"\n",
    "    st = StanfordNERTagger(cla,jar)\n",
    "    ner = st.tag(text.split())\n",
    "    after = \"\"\n",
    "    for item in ner:\n",
    "        if item[1] == 'O':\n",
    "            after += item[0] + ' '\n",
    "        else:\n",
    "            after += item[1] + ' '\n",
    "    return after\n",
    "\n",
    "def posTagging(text):\n",
    "    jar = 'C:\\\\Users\\\\lxykh\\\\OneDrive\\\\Desktop\\\\4780\\\\2018Fall\\\\kaggle\\\\stanford-postagger-2018-10-16\\\\stanford-postagger.jar'\n",
    "    model = \"C:\\\\Users\\\\lxykh\\\\OneDrive\\\\Desktop\\\\4780\\\\2018Fall\\\\kaggle\\\\stanford-postagger-2018-10-16\\\\models\\\\english-bidirectional-distsim.tagger\"\n",
    "    st = StanfordPOSTagger(model,jar)\n",
    "    pos = st.tag(text.split())\n",
    "    after = \"\"\n",
    "    for item in pos:\n",
    "        after += item[1] + \" \"\n",
    "    return after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"`` : NN NNS VBZ IN JJ NNP NNP NNP NNP '' \""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "\n",
    "def pos_tagging(text):\n",
    "    '''\n",
    "    Takes a string of words and returns a string with parts-of-speech of words\n",
    "    INPUT: string\n",
    "    OUTPUT: string\n",
    "    '''\n",
    "    pos = pos_tag(word_tokenize(text))\n",
    "    string = \"\"\n",
    "    for item in pos:\n",
    "        string += item[1] + \" \"\n",
    "    return string\n",
    "\n",
    "pos_tagging(data.loc[294,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def preprocess(sentence,stop_words):\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    translator=sentence.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    translator = translator.lower()\n",
    "    tokens = word_tokenize(translator)\n",
    "    tokens = [item for item in tokens if item not in stop_words]\n",
    "    final = [lemma.lemmatize(tagged_word) for tagged_word in tokens]\n",
    "    return \" \".join(final)\n",
    "\n",
    "# tokenize the email and hashes the symbols into a vector\n",
    "def extractfeaturesnaive(tweet, B):\n",
    "    v = np.zeros(B)\n",
    "    tokens = tweet.split()\n",
    "    for token in tokens:\n",
    "        v[hash(token) % B] = 1\n",
    "    return v\n",
    "\n",
    "def loadspamdata(data, B=128):\n",
    "    alltweets = [x for x in data.text]    \n",
    "    xs = np.zeros((len(alltweets), B))\n",
    "    ys = np.zeros(len(alltweets))\n",
    "    for i,line in enumerate(alltweets):\n",
    "        xs[i, :] = extractfeaturesnaive(line, B)\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_of_day(df, timestamp):\n",
    "    '''\n",
    "    Takes a DataFrame and a specified column containing a timestamp and creates\n",
    "    a new column indicating the hour of the day\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame with one new column\n",
    "    '''\n",
    "    new_df = df.copy()\n",
    "    new_df['hour'] = new_df[timestamp].dt.hour\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def period_of_day(df, timestamp):\n",
    "    '''\n",
    "    Takes a DataFrame and a specified column containing a timestamp and creates\n",
    "    a new column indicating the period of the day in 6-hour increments\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame with one new column\n",
    "    '''\n",
    "    new_df = df.copy()\n",
    "    new_df['hour_20_02'] = np.where(((new_df[timestamp].dt.hour >= 20) |(new_df[timestamp].dt.hour < 2)),True, False)\n",
    "    new_df['hour_14_20'] = np.where(((new_df[timestamp].dt.hour >= 14) &(new_df[timestamp].dt.hour < 20)),True, False)\n",
    "    new_df['hour_08_14'] = np.where(((new_df[timestamp].dt.hour >= 8) &(new_df[timestamp].dt.hour < 14)),True, False)\n",
    "    new_df['hour_02_08'] = np.where(((new_df[timestamp].dt.hour >= 2) &(new_df[timestamp].dt.hour < 8)),True, False)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def day_of_week(df, timestamp):\n",
    "    '''\n",
    "    Takes a DataFrame and a specified column containing a timestamp and creates\n",
    "    a new column indicating the day of the week\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame with one new column\n",
    "    '''\n",
    "    new_df = df.copy()\n",
    "    new_df['day_of_week'] = new_df[timestamp].dt.weekday\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def weekend(df, day_of_week):\n",
    "    '''\n",
    "    Takes a DataFrame and a specified column containing a day of the week and\n",
    "    creates a new column indicating if the day occurs on a weekend\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame with one new column\n",
    "    '''\n",
    "    new_df = df.copy()\n",
    "    new_df['weekend'] = new_df[day_of_week].apply(lambda x: 1 if x in [5, 6] else 0)\n",
    "    return new_df\n",
    "\n",
    "def get_vader_scores(text):\n",
    "    '''\n",
    "    Takes a string of text and outputs four values for Vader's negative,\n",
    "    neutral, positive, and compound (normalized) sentiment scores\n",
    "    INPUT: a string\n",
    "    OUTPUT: a dictionary of four sentiment scores\n",
    "    '''\n",
    "\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    return analyser.polarity_scores(text)\n",
    "\n",
    "\n",
    "def apply_vader(df, column):\n",
    "    '''\n",
    "    Takes a DataFrame with a specified column of text and adds four new columns\n",
    "    to the DataFrame, corresponding to the Vader sentiment scores\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame with four additional columns\n",
    "    '''\n",
    "\n",
    "    sentiment = pd.DataFrame(df[column].apply(get_vader_scores))\n",
    "    unpacked = pd.DataFrame([d for idx, d in sentiment[column].iteritems()],\n",
    "                            index=sentiment.index)\n",
    "    unpacked['compound'] += 1\n",
    "    columns = {'neu': 'v_neutral', 'pos': 'v_positive', 'neg': 'v_negative'}\n",
    "    unpacked.rename(columns=columns, inplace=True)\n",
    "    return pd.concat([df, unpacked], axis=1)\n",
    "\n",
    "def sentence_word_length(text):\n",
    "    '''\n",
    "    Finds the average length of sentences and words in a given text\n",
    "    INPUT: string\n",
    "    OUTPUT: float(average sentence length), float(average word length)\n",
    "    '''\n",
    "\n",
    "    sentence_lengths = []\n",
    "    word_lengths = []\n",
    "    sentences = [s.strip() for s in re.split('[\\.\\?!]', text) if s]\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        word_lengths = word_lengths + [len(word) for word in words]\n",
    "        sentence_length = len(words)\n",
    "        sentence_lengths.append(sentence_length)\n",
    "    return (sum(sentence_lengths) / float(len(sentence_lengths)),\n",
    "            sum(word_lengths) / float(len(word_lengths)))\n",
    "\n",
    "\n",
    "def apply_avg_lengths(df, column):\n",
    "    '''\n",
    "    Takes a DataFrame with a specified column of text and adds two new columns\n",
    "    to the DataFrame, corresponding to the average sentence and word lengths\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame with two additional columns\n",
    "    '''\n",
    "\n",
    "    avg_lengths = pd.DataFrame(df[column].apply(sentence_word_length))\n",
    "    unpacked = pd.DataFrame([d for idx, d in avg_lengths[column].iteritems()],\n",
    "                            index=avg_lengths.index)\n",
    "    unpacked.columns = ['avg_sentence_length', 'avg_word_length']\n",
    "    return pd.concat([df, unpacked], axis=1)\n",
    "\n",
    "\n",
    "def tweet_length(df, column):\n",
    "    '''\n",
    "    Takes a DataFrame and the name of a column of text and creates a new\n",
    "    column containing the count of characters of the text\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame, with one new column\n",
    "    '''\n",
    "\n",
    "    new_df = df.copy()\n",
    "    new_df['tweet_length'] = new_df[column].str.len()\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def count_character(text, character):\n",
    "    '''\n",
    "    Takes a text string and a character and outputs the number of occurances\n",
    "    of that character in the text\n",
    "    INPUT: text string, character string\n",
    "    OUTPUT: int\n",
    "    '''\n",
    "\n",
    "    return text.count(character)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "p.set_options(p.OPT.URL)\n",
    "data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "data = tweet_length(data, 'text')\n",
    "data = apply_avg_lengths(data, 'text')\n",
    "i = 0\n",
    "for text in data.text:\n",
    "    parsed = p.parse(text)\n",
    "    if parsed.urls is not None:\n",
    "        data.loc[i,'url'] = 1\n",
    "    else: data.loc[i,'url'] = 0\n",
    "    if parsed.hashtags is not None:\n",
    "        data.loc[i,'hashtag'] = 1\n",
    "    else: data.loc[i,'hashtag'] = 0\n",
    "    if parsed.mentions is not None:\n",
    "        data.loc[i,'mentions'] = 1\n",
    "    else: data.loc[i,'mentions'] = 0\n",
    "    text = p.clean(text)\n",
    "    #data.loc[i,'text'] = preprocess(text,stop_words)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data\n",
    "predicted = gb.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.DataFrame(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.to_csv(\"predicted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.0, 6.416666666666667)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def sentence_word_length(text):\n",
    "    '''\n",
    "    Finds the average length of sentences and words in a given text\n",
    "    INPUT: string\n",
    "    OUTPUT: float(average sentence length), float(average word length)\n",
    "    '''\n",
    "\n",
    "    sentence_lengths = []\n",
    "    word_lengths = []\n",
    "    sentences = [s.strip() for s in re.split('[\\.\\?!]', text) if s]\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        word_lengths = word_lengths + [len(word) for word in words]\n",
    "        sentence_length = len(words)\n",
    "        sentence_lengths.append(sentence_length)\n",
    "    return (sum(sentence_lengths) / float(len(sentence_lengths)),\n",
    "            sum(word_lengths) / float(len(word_lengths)))\n",
    "sentence_word_length(data.loc[3,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'thought prayer victim family along everyone berrien county courthouse st joseph michigan'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[3,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    '''\n",
    "    Takes a string of words and returns a string with parts-of-speech of words\n",
    "    INPUT: string\n",
    "    OUTPUT: string\n",
    "    '''\n",
    "    pos = pos_tag(word_tokenize(text))\n",
    "    string = \"\"\n",
    "    for item in pos:\n",
    "        string += item[1] + \" \"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lxykh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NNP NNP NNP NNP . NN : JJ NN : NN '"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "pos_tagging(data.loc[11,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "def ner_tagging(text):\n",
    "    '''\n",
    "    Takes a tweetokenized string of words and uses the Stanford NER Tagger to\n",
    "    replace names, places, and organizations with a standard token\n",
    "    INPUT: string\n",
    "    OUTPUT: string\n",
    "    '''\n",
    "    st = StanfordNERTagger('C:\\Python27\\Lib\\stanford-ner\\classifiers\\english.all.3class.distsim.crf.ser.gz',\\\n",
    "                           'C:\\Python27\\Lib\\stanford-ner\\stanford-ner.jar')\n",
    "    ner = st.tag(word_tokenize(text))\n",
    "    string = \"\"\n",
    "    for item in ner:\n",
    "        if item[1] == 'O':\n",
    "            if item[0] == '<' or item[0] == '@':\n",
    "                string += item[0]\n",
    "            elif item[0] == '>':\n",
    "                    string = string[:-1] + item[0] + ' '\n",
    "            else:\n",
    "                string += item[0] + ' '\n",
    "        else:\n",
    "            string += item[1] + ' '\n",
    "    tweet = ''\n",
    "    for word in string.split():\n",
    "        if word.isupper():\n",
    "            tweet += word + ' '\n",
    "        else:\n",
    "            tweet += word.lower() + ' '\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123.62358642972536"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "np.mean(data.loc[data['label']==1,'text'].apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103.26382978723404"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data.loc[data['label']==-1,'text'].apply(len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
