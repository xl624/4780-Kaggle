{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from collections import Counter\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import datetime\n",
    "from vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "import re\n",
    "from nltk.tag import StanfordPOSTagger, StanfordNERTagger\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average num char when y = 1: ', 123.62358642972536)\n",
      "('Average num char when y = -1: ', 103.26382978723404)\n",
      "('Average num words when y = 1: ', 20.248788368336026)\n",
      "('Average num words when y = -1: ', 13.476595744680852)\n",
      "('Average word length when y = 1: ', 5.420011027379853)\n",
      "('Average word length when y = -1: ', 8.05688745871895)\n",
      "('Average num uppercase word when y = 1: ', 0.5977382875605816)\n",
      "('Average num uppercase word when y = -1: ', 0.44042553191489364)\n",
      "('Percent whole sentence upper when y = 1: ', 0.01938610662358643)\n",
      "('Percent whole sentence upper when y = -1: ', 0.09148936170212765)\n",
      "('Average num url when y = 1: ', 0.0888529886914378)\n",
      "('Average num url when y = -1: ', 0.8957446808510638)\n",
      "('Average num mention when y = 1: ', 0.9612277867528272)\n",
      "('Average num mention when y = -1: ', 0.2680851063829787)\n",
      "('Average num hashtag when y = 1: ', 0.13408723747980614)\n",
      "('Average num hashtag when y = -1: ', 0.9595744680851064)\n",
      "('Average num number when y = 1: ', 0.18739903069466882)\n",
      "('Average num number when y = -1: ', 0.21914893617021278)\n",
      "('Average num smiley when y = 1: ', 0.022617124394184167)\n",
      "('Average num smiley when y = -1: ', 0.010638297872340425)\n",
      "('Average num start with \" when y = 1: ', 0.3182552504038772)\n",
      "('Average num start with \" when y = -1: ', 0.006382978723404255)\n",
      "('Average num start with @ when y = 1: ', 0.004846526655896607)\n",
      "('Average num start with @ when y = -1: ', 0.0)\n",
      "('Average num start with # when y = 1: ', 0.0)\n",
      "('Average num start with # when y = -1: ', 0.059574468085106386)\n",
      "('Average num ; when y = 1: ', 0.7512116316639742)\n",
      "('Average num ; when y = -1: ', 0.5617021276595745)\n",
      "('Average num ! when y = 1: ', 0.7996768982229402)\n",
      "('Average num ! when y = -1: ', 0.8723404255319149)\n",
      "('Average num \" when y = 1: ', 0.7140549273021002)\n",
      "('Average num \" when y = -1: ', 0.0425531914893617)\n",
      "('Average num ... when y = 1: ', 0.011308562197092083)\n",
      "('Average num ... when y = -1: ', 0.00425531914893617)\n",
      "('Average num . when y = 1: ', 1.3602584814216478)\n",
      "('Average num . when y = -1: ', 1.4829787234042553)\n",
      "('Average num ? when y = 1: ', 0.061389337641357025)\n",
      "('Average num ? when y = -1: ', 0.06808510638297872)\n",
      "('Average favorite when y = 1: ', 15923.681744749596)\n",
      "('Average favorite when y = -1: ', 15381.478723404256)\n",
      "('Average retweet when y = 1: ', 5269.670436187399)\n",
      "('Average retweet when y = -1: ', 5630.444680851064)\n",
      "('Average id when y = 1: ', 541.2342487883683)\n",
      "('Average id when y = -1: ', 547.6425531914894)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-48096fc39cb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average id when y = 1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average id when y = -1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average stopword when y = 1: \"\u001b[0m\u001b[1;33m,\u001b[0m       \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average stopword when y = -1: \"\u001b[0m\u001b[1;33m,\u001b[0m       \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3192\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3194\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3196\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-48096fc39cb9>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average id when y = 1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average id when y = -1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average stopword when y = 1: \"\u001b[0m\u001b[1;33m,\u001b[0m       \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average stopword when y = -1: \"\u001b[0m\u001b[1;33m,\u001b[0m       \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "p.set_options(p.OPT.URL, p.OPT.EMOJI,p.OPT.MENTION,p.OPT.HASHTAG,p.OPT.RESERVED,p.OPT.SMILEY,p.OPT.NUMBER)\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "data['tokenized'] = data['text'].apply(p.tokenize)\n",
    "print(\"Average num char when y = 1: \", np.mean(data.loc[data['label']==1,'text'].apply(len)))\n",
    "print(\"Average num char when y = -1: \", np.mean(data.loc[data['label']==-1,'text'].apply(len)))\n",
    "print(\"Average num words when y = 1: \", np.mean(data.loc[data['label']==1,'text'].apply(lambda x: len(x.split()))))\n",
    "print(\"Average num words when y = -1: \", np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: len(x.split()))))\n",
    "print(\"Average word length when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'text'].apply(lambda x: np.mean([len(a) for a in x.split()]))))\n",
    "print(\"Average word length when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: np.mean([len(a) for a in x.split()]))))\n",
    "print(\"Average num uppercase word when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'text'].apply(lambda x: len(re.findall('\\s([A-Z][A-Z]+)', x)))))\n",
    "print(\"Average num uppercase word when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: len(re.findall('\\s([A-Z][A-Z]+)', x)))))\n",
    "print(\"Percent whole sentence upper when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'tokenized'].apply(lambda x: x.isupper())))\n",
    "print(\"Percent whole sentence upper when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'tokenized'].apply(lambda x: x.isupper())))\n",
    "print(\"Average num url when y = 1: \", np.mean(data.loc[data['label']==1,'tokenized'].apply(lambda x: x.count('$URL$'))))\n",
    "print(\"Average num url when y = -1: \", np.mean(data.loc[data['label']==-1,'tokenized'].apply(lambda x: x.count('$URL$'))))\n",
    "print(\"Average num mention when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'tokenized'].apply(lambda x: x.count('$MENTION$'))))\n",
    "print(\"Average num mention when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'tokenized'].apply(lambda x: x.count('$MENTION$'))))\n",
    "print(\"Average num hashtag when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'tokenized'].apply(lambda x: x.count('$HASHTAG$'))))\n",
    "print(\"Average num hashtag when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'tokenized'].apply(lambda x: x.count('$HASHTAG$'))))\n",
    "print(\"Average num number when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'tokenized'].apply(lambda x: x.count('$NUMBER$'))))\n",
    "print(\"Average num number when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'tokenized'].apply(lambda x: x.count('$NUMBER$'))))\n",
    "print(\"Average num smiley when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'tokenized'].apply(lambda x: x.count('$SMILEY$'))))\n",
    "print(\"Average num smiley when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'tokenized'].apply(lambda x: x.count('$SMILEY$'))))\n",
    "print(\"Average num start with \\\" when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'tokenized'].apply(lambda x: len(re.findall('^\"', x)))))\n",
    "print(\"Average num start with \\\" when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'tokenized'].apply(lambda x: len(re.findall('^\"', x)))))\n",
    "print(\"Average num start with @ when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'text'].apply(lambda x: len(re.findall('^@', x)))))\n",
    "print(\"Average num start with @ when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: len(re.findall('^@', x)))))\n",
    "print(\"Average num start with # when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'text'].apply(lambda x: len(re.findall('^#', x)))))\n",
    "print(\"Average num start with # when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: len(re.findall('^#', x)))))\n",
    "print(\"Average num ; when y = 1: \", np.mean(data.loc[data['label']==1,'text'].apply(lambda x: x.count(\";\"))))\n",
    "print(\"Average num ; when y = -1: \", np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: x.count(\";\"))))\n",
    "print(\"Average num ! when y = 1: \", np.mean(data.loc[data['label']==1,'text'].apply(lambda x: x.count(\"!\"))))\n",
    "print(\"Average num ! when y = -1: \", np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: x.count(\"!\"))))\n",
    "print(\"Average num \\\" when y = 1: \", np.mean(data.loc[data['label']==1,'text'].apply(lambda x: x.count(\"\\\"\"))))\n",
    "print(\"Average num \\\" when y = -1: \", np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: x.count(\"\\\"\"))))\n",
    "print(\"Average num ... when y = 1: \", np.mean(data.loc[data['label']==1,'text'].apply(lambda x: x.count(\"...\"))))\n",
    "print(\"Average num ... when y = -1: \", np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: x.count(\"...\"))))\n",
    "print(\"Average num . when y = 1: \", np.mean(data.loc[data['label']==1,'text'].apply(lambda x: x.count(\".\"))))\n",
    "print(\"Average num . when y = -1: \", np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: x.count(\".\"))))\n",
    "print(\"Average num ? when y = 1: \", np.mean(data.loc[data['label']==1,'text'].apply(lambda x: x.count(\"?\"))))\n",
    "print(\"Average num ? when y = -1: \", np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: x.count(\"?\"))))\n",
    "print(\"Average favorite when y = 1: \", np.mean(data.loc[data['label']==1,'favoriteCount']))\n",
    "print(\"Average favorite when y = -1: \", np.mean(data.loc[data['label']==-1,'favoriteCount']))\n",
    "print(\"Average retweet when y = 1: \", np.mean(data.loc[data['label']==1,'retweetCount']))\n",
    "print(\"Average retweet when y = -1: \", np.mean(data.loc[data['label']==-1,'retweetCount']))\n",
    "print(\"Average id when y = 1: \", np.mean(data.loc[data['label']==1,'id']))\n",
    "print(\"Average id when y = -1: \", np.mean(data.loc[data['label']==-1,'id']))\n",
    "print(\"Average stopword when y = 1: \", \\\n",
    "      np.mean(data.loc[data['label']==1,'text'].apply(lambda x: sum([a in stop_words for a in x.split()]))))\n",
    "print(\"Average stopword when y = -1: \", \\\n",
    "      np.mean(data.loc[data['label']==-1,'text'].apply(lambda x: sum([a in stop_words for a in x.split()]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "data1 = pd.read_csv(\"test.csv\")\n",
    "data1['statusSource'] = 0\n",
    "data = pd.concat([data,data1],axis=0)\n",
    "data.index = np.arange(0,len(data))\n",
    "def preprocess(data):\n",
    "    data['created']=pd.to_datetime(data['created'], errors='coerce')\n",
    "    data['dayInWeek'] = data['created'].dt.weekday\n",
    "    data['hourInDay'] = data['created'].dt.hour\n",
    "    data['tokenized'] = data['text'].apply(p.tokenize)\n",
    "    data['numchar'] = data['text'].apply(len)\n",
    "    data['numWords'] = data['text'].apply(lambda x: len(x.split()))\n",
    "    data['avgWordLen'] =data['text'].apply(lambda x: np.mean([len(a) for a in x.split()]))\n",
    "    data['wholeUpper'] = data['tokenized'].apply(lambda x: np.sum(x.isupper()))\n",
    "    data['numURL'] = data['tokenized'].apply(lambda x: x.count('$URL$'))\n",
    "    data['numHash'] = data['tokenized'].apply(lambda x: x.count('$HASHTAG$'))\n",
    "    data['mention'] = data['tokenized'].apply(lambda x: x.count('$MENTION$'))\n",
    "    data['startQuote']=data['tokenized'].apply(lambda x: len(re.findall('^\"', x)))\n",
    "    data['startMention']=data['tokenized'].apply(lambda x: len(re.findall('^@', x)))\n",
    "    data['startHash']=data['tokenized'].apply(lambda x: len(re.findall('^#', x)))\n",
    "    data['numComa']=data['text'].apply(lambda x: x.count(\";\"))\n",
    "    data['numQuoSym']=data['text'].apply(lambda x: x.count(\"\\\"\"))\n",
    "    data['numSmiley'] = data['tokenized'].apply(lambda x: x.count('$SMILEY$'))\n",
    "    data['numStopWord']=data['text'].apply(lambda x: sum([a in stop_words for a in x.split()]))\n",
    "    data['wordLength']=data['text'].apply(lambda x: np.mean([len(a) for a in x.split()]))\n",
    "    data['posTagged']=data['tokenized'].apply(posTagging)\n",
    "    data['nerTagged']=data['tokenized'].apply(nerTagging)\n",
    "    return data\n",
    "\n",
    "def nerTagging(text):\n",
    "    jar = \"stanford-ner-2018-10-16\\\\stanford-ner.jar\"\n",
    "    cla = \"stanford-ner-2018-10-16\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz\"\n",
    "    st = StanfordNERTagger(cla,jar)\n",
    "    ner = st.tag(text.split())\n",
    "    after = \"\"\n",
    "    for item in ner:\n",
    "        if item[1] == 'O':\n",
    "            after += item[0] + ' '\n",
    "        else:\n",
    "            after += item[1] + ' '\n",
    "    return after\n",
    "\n",
    "def posTagging(text):\n",
    "    jar = 'stanford-postagger-2018-10-16\\\\stanford-postagger.jar'\n",
    "    model = \"stanford-postagger-2018-10-16\\\\models\\\\english-bidirectional-distsim.tagger\"\n",
    "    st = StanfordPOSTagger(model,jar)\n",
    "    pos = st.tag(text.split())\n",
    "    after = \"\"\n",
    "    for item in pos:\n",
    "        after += item[1] + \" \"\n",
    "    return after\n",
    "# data = apply_vader(data, 'text')\n",
    "# data['created']=pd.to_datetime(data['created'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the java file!\nUse software specific configuration paramaters or set the JAVAHOME environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-70deb96afcd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-7ed80a3b05df>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'numStopWord'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'wordLength'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posTagged'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposTagging\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nerTagged'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnerTagging\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3192\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3194\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3196\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-7ed80a3b05df>\u001b[0m in \u001b[0;36mposTagging\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"stanford-postagger-2018-10-16\\\\models\\\\english-bidirectional-distsim.tagger\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStanfordPOSTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mjar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mafter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\nltk\\tag\\stanford.pyc\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;31m# This function should return list of tuple rather than list of list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\nltk\\tag\\stanford.pyc\u001b[0m in \u001b[0;36mtag_sents\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mdefault_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_java_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mconfig_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# Create a temporary input file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\nltk\\internals.pyc\u001b[0m in \u001b[0;36mconfig_java\u001b[1;34m(bin, options, verbose)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0menv_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'JAVAHOME'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'JAVA_HOME'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mbinary_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'java.exe'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     )\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\nltk\\internals.pyc\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    694\u001b[0m     return next(\n\u001b[0;32m    695\u001b[0m         find_binary_iter(\n\u001b[1;32m--> 696\u001b[1;33m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m         )\n\u001b[0;32m    698\u001b[0m     )\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\nltk\\internals.pyc\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \"\"\"\n\u001b[0;32m    679\u001b[0m     for file in find_file_iter(\n\u001b[1;32m--> 680\u001b[1;33m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m     ):\n\u001b[0;32m    682\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\nltk\\internals.pyc\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'\\n\\n  For more information on %s, see:\\n    <%s>'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the java file!\nUse software specific configuration paramaters or set the JAVAHOME environment variable.\n==========================================================================="
     ]
    }
   ],
   "source": [
    "data = preprocess(data)\n",
    "data = pd.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"data.csv\")\n",
    "text = TfidfVectorizer(ngram_range=(1, 2))\n",
    "a = text.fit_transform(data['text'])\n",
    "colnames = text.get_feature_names()\n",
    "a.todense().shape\n",
    "idx = data.index\n",
    "text = pd.DataFrame(a.todense(),columns=[colnames],index=idx)\n",
    "\n",
    "pos = TfidfVectorizer(ngram_range=(1, 3))\n",
    "a = pos.fit_transform(data['posTagged'])\n",
    "colnames = pos.get_feature_names()\n",
    "a.todense().shape\n",
    "idx = data.index\n",
    "pos = pd.DataFrame(a.todense(),columns=[colnames],index=idx)\n",
    "\n",
    "ner = TfidfVectorizer(ngram_range=(1, 2))\n",
    "a = ner.fit_transform(data['nerTagged'])\n",
    "colnames = ner.get_feature_names()\n",
    "a.todense().shape\n",
    "idx = data.index\n",
    "pos = pd.DataFrame(a.todense(),columns=[colnames],index=idx)\n",
    "\n",
    "data = pd.concat([data,text,pos],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"nerTagged\"],axis=1)\n",
    "X = X.drop(['label'], axis=1)\n",
    "X=X.drop(['favorited', 'replyToSID', 'truncated','screenName','isRetweet','retweeted'],axis=1)\n",
    "X=X.drop(['id', 'text', 'favoriteCount','replyToSN','created','id.1','replyToUID',\\\n",
    "               \"longitude\",'latitude','tokenized','posTagged'],axis=1)\n",
    "X = X.drop(['statusSource'],axis=1)\n",
    "Y = pd.DataFrame(np.where(data['label'] == 1, 1,0))\n",
    "X = np.squeeze(X.values)\n",
    "X = normalize(X)\n",
    "Y = Y.values.flatten()\n",
    "cut = 1089\n",
    "X_train = X[:cut,]\n",
    "Y_train = Y[:cut]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=200,\n",
    "                                        learning_rate=.1,\n",
    "                                        max_depth=6,\n",
    "                                        min_samples_split=2,\n",
    "                                        min_samples_leaf=1,\n",
    "                                        subsample=1,\n",
    "                                        max_features=None\n",
    "                                        ).fit(X_train, Y_train)\n",
    "X_valid = X[cut:,]\n",
    "predicted = gb.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.DataFrame(predicted)\n",
    "predicted.to_csv(\"predicted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8955465ad693>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStanfordNERTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcla\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mjar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"I am happy\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\nltk\\tag\\stanford.pyc\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;31m# This function should return list of tuple rather than list of list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\nltk\\tag\\stanford.pyc\u001b[0m in \u001b[0;36mtag_sents\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Run the tagger and get the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         stanpos_output, _stderr = java(\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasspath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stanford_jar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         )\n\u001b[0;32m    118\u001b[0m         \u001b[0mstanpos_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstanpos_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\nltk\\internals.pyc\u001b[0m in \u001b[0;36mjava\u001b[1;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;31m# Check the return code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\subprocess.pyc\u001b[0m in \u001b[0;36mcommunicate\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\subprocess.pyc\u001b[0m in \u001b[0;36m_communicate\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m                 \u001b[0mstdout_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m                 \u001b[0mstderr_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\threading.pyc\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__stopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__block\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s.join(): thread stopped\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s.wait(): got it\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_171/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "jar = \"stanford-ner-2018-10-16\\\\stanford-ner.jar\"\n",
    "cla = \"stanford-ner-2018-10-16\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz\"\n",
    "st = StanfordNERTagger(cla,jar)\n",
    "text = \"I am happy\"\n",
    "ner = st.tag(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nerTagging(text):\n",
    "    jar = \"stanford-ner-2018-10-16\\\\stanford-ner.jar\"\n",
    "    cla = \"stanford-ner-2018-10-16\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz\"\n",
    "    st = StanfordNERTagger(cla,jar)\n",
    "    ner = st.tag(text.split())\n",
    "    after = \"\"\n",
    "    for item in ner:\n",
    "        if item[1] == 'O':\n",
    "            after += item[0] + ' '\n",
    "        else:\n",
    "            after += item[1] + ' '\n",
    "    return after\n",
    "\n",
    "def posTagging(text):\n",
    "    jar = 'stanford-postagger-2018-10-16\\\\stanford-postagger.jar'\n",
    "    model = \"stanford-postagger-2018-10-16\\\\models\\\\english-bidirectional-distsim.tagger\"\n",
    "    st = StanfordPOSTagger(model,jar)\n",
    "    pos = st.tag(text.split())\n",
    "    after = \"\"\n",
    "    for item in pos:\n",
    "        after += item[1] + \" \"\n",
    "    return after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"`` : NN NNS VBZ IN JJ NNP NNP NNP NNP '' \""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "\n",
    "def pos_tagging(text):\n",
    "    '''\n",
    "    Takes a string of words and returns a string with parts-of-speech of words\n",
    "    INPUT: string\n",
    "    OUTPUT: string\n",
    "    '''\n",
    "    pos = pos_tag(word_tokenize(text))\n",
    "    string = \"\"\n",
    "    for item in pos:\n",
    "        string += item[1] + \" \"\n",
    "    return string\n",
    "\n",
    "pos_tagging(data.loc[294,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def preprocess(sentence,stop_words):\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    translator=sentence.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    translator = translator.lower()\n",
    "    tokens = word_tokenize(translator)\n",
    "    tokens = [item for item in tokens if item not in stop_words]\n",
    "    final = [lemma.lemmatize(tagged_word) for tagged_word in tokens]\n",
    "    return \" \".join(final)\n",
    "\n",
    "# tokenize the email and hashes the symbols into a vector\n",
    "def extractfeaturesnaive(tweet, B):\n",
    "    v = np.zeros(B)\n",
    "    tokens = tweet.split()\n",
    "    for token in tokens:\n",
    "        v[hash(token) % B] = 1\n",
    "    return v\n",
    "\n",
    "def loadspamdata(data, B=128):\n",
    "    alltweets = [x for x in data.text]    \n",
    "    xs = np.zeros((len(alltweets), B))\n",
    "    ys = np.zeros(len(alltweets))\n",
    "    for i,line in enumerate(alltweets):\n",
    "        xs[i, :] = extractfeaturesnaive(line, B)\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_of_day(df, timestamp):\n",
    "    '''\n",
    "    Takes a DataFrame and a specified column containing a timestamp and creates\n",
    "    a new column indicating the hour of the day\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame with one new column\n",
    "    '''\n",
    "    new_df = df.copy()\n",
    "    new_df['hour'] = new_df[timestamp].dt.hour\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def period_of_day(df, timestamp):\n",
    "    '''\n",
    "    Takes a DataFrame and a specified column containing a timestamp and creates\n",
    "    a new column indicating the period of the day in 6-hour increments\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame with one new column\n",
    "    '''\n",
    "    new_df = df.copy()\n",
    "    new_df['hour_20_02'] = np.where(((new_df[timestamp].dt.hour >= 20) |(new_df[timestamp].dt.hour < 2)),True, False)\n",
    "    new_df['hour_14_20'] = np.where(((new_df[timestamp].dt.hour >= 14) &(new_df[timestamp].dt.hour < 20)),True, False)\n",
    "    new_df['hour_08_14'] = np.where(((new_df[timestamp].dt.hour >= 8) &(new_df[timestamp].dt.hour < 14)),True, False)\n",
    "    new_df['hour_02_08'] = np.where(((new_df[timestamp].dt.hour >= 2) &(new_df[timestamp].dt.hour < 8)),True, False)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def day_of_week(df, timestamp):\n",
    "    '''\n",
    "    Takes a DataFrame and a specified column containing a timestamp and creates\n",
    "    a new column indicating the day of the week\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame with one new column\n",
    "    '''\n",
    "    new_df = df.copy()\n",
    "    new_df['day_of_week'] = new_df[timestamp].dt.weekday\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def weekend(df, day_of_week):\n",
    "    '''\n",
    "    Takes a DataFrame and a specified column containing a day of the week and\n",
    "    creates a new column indicating if the day occurs on a weekend\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame with one new column\n",
    "    '''\n",
    "    new_df = df.copy()\n",
    "    new_df['weekend'] = new_df[day_of_week].apply(lambda x: 1 if x in [5, 6] else 0)\n",
    "    return new_df\n",
    "\n",
    "def get_vader_scores(text):\n",
    "    '''\n",
    "    Takes a string of text and outputs four values for Vader's negative,\n",
    "    neutral, positive, and compound (normalized) sentiment scores\n",
    "    INPUT: a string\n",
    "    OUTPUT: a dictionary of four sentiment scores\n",
    "    '''\n",
    "\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    return analyser.polarity_scores(text)\n",
    "\n",
    "\n",
    "def apply_vader(df, column):\n",
    "    sentiment = pd.DataFrame(df[column].apply(get_vader_scores))\n",
    "    unpacked = pd.DataFrame([d for idx, d in sentiment[column].iteritems()],\n",
    "                            index=sentiment.index)\n",
    "    unpacked['compound'] += 1\n",
    "    columns = {'neu': 'v_neutral', 'pos': 'v_positive', 'neg': 'v_negative'}\n",
    "    unpacked.rename(columns=columns, inplace=True)\n",
    "    return pd.concat([df, unpacked], axis=1)\n",
    "\n",
    "def sentence_word_length(text):\n",
    "    '''\n",
    "    Finds the average length of sentences and words in a given text\n",
    "    INPUT: string\n",
    "    OUTPUT: float(average sentence length), float(average word length)\n",
    "    '''\n",
    "\n",
    "    sentence_lengths = []\n",
    "    word_lengths = []\n",
    "    sentences = [s.strip() for s in re.split('[\\.\\?!]', text) if s]\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        word_lengths = word_lengths + [len(word) for word in words]\n",
    "        sentence_length = len(words)\n",
    "        sentence_lengths.append(sentence_length)\n",
    "    return (sum(sentence_lengths) / float(len(sentence_lengths)),\n",
    "            sum(word_lengths) / float(len(word_lengths)))\n",
    "\n",
    "\n",
    "def apply_avg_lengths(df, column):\n",
    "    '''\n",
    "    Takes a DataFrame with a specified column of text and adds two new columns\n",
    "    to the DataFrame, corresponding to the average sentence and word lengths\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame with two additional columns\n",
    "    '''\n",
    "\n",
    "    avg_lengths = pd.DataFrame(df[column].apply(sentence_word_length))\n",
    "    unpacked = pd.DataFrame([d for idx, d in avg_lengths[column].iteritems()],\n",
    "                            index=avg_lengths.index)\n",
    "    unpacked.columns = ['avg_sentence_length', 'avg_word_length']\n",
    "    return pd.concat([df, unpacked], axis=1)\n",
    "\n",
    "\n",
    "def tweet_length(df, column):\n",
    "    '''\n",
    "    Takes a DataFrame and the name of a column of text and creates a new\n",
    "    column containing the count of characters of the text\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame, with one new column\n",
    "    '''\n",
    "\n",
    "    new_df = df.copy()\n",
    "    new_df['tweet_length'] = new_df[column].str.len()\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def count_character(text, character):\n",
    "    '''\n",
    "    Takes a text string and a character and outputs the number of occurances\n",
    "    of that character in the text\n",
    "    INPUT: text string, character string\n",
    "    OUTPUT: int\n",
    "    '''\n",
    "\n",
    "    return text.count(character)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "p.set_options(p.OPT.URL)\n",
    "a = pd.read_csv(\"test.csv\")\n",
    "sum([x in stop_words for x in a.loc[0,'text'].split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.DataFrame(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.to_csv(\"predicted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.0, 6.416666666666667)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def sentence_word_length(text):\n",
    "    '''\n",
    "    Finds the average length of sentences and words in a given text\n",
    "    INPUT: string\n",
    "    OUTPUT: float(average sentence length), float(average word length)\n",
    "    '''\n",
    "\n",
    "    sentence_lengths = []\n",
    "    word_lengths = []\n",
    "    sentences = [s.strip() for s in re.split('[\\.\\?!]', text) if s]\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        word_lengths = word_lengths + [len(word) for word in words]\n",
    "        sentence_length = len(words)\n",
    "        sentence_lengths.append(sentence_length)\n",
    "    return (sum(sentence_lengths) / float(len(sentence_lengths)),\n",
    "            sum(word_lengths) / float(len(word_lengths)))\n",
    "sentence_word_length(data.loc[3,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'thought prayer victim family along everyone berrien county courthouse st joseph michigan'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[3,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    '''\n",
    "    Takes a string of words and returns a string with parts-of-speech of words\n",
    "    INPUT: string\n",
    "    OUTPUT: string\n",
    "    '''\n",
    "    pos = pos_tag(word_tokenize(text))\n",
    "    string = \"\"\n",
    "    for item in pos:\n",
    "        string += item[1] + \" \"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lxykh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NNP NNP NNP NNP . NN : JJ NN : NN '"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "pos_tagging(data.loc[11,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "def ner_tagging(text):\n",
    "    '''\n",
    "    Takes a tweetokenized string of words and uses the Stanford NER Tagger to\n",
    "    replace names, places, and organizations with a standard token\n",
    "    INPUT: string\n",
    "    OUTPUT: string\n",
    "    '''\n",
    "    st = StanfordNERTagger('C:\\Python27\\Lib\\stanford-ner\\classifiers\\english.all.3class.distsim.crf.ser.gz',\\\n",
    "                           'C:\\Python27\\Lib\\stanford-ner\\stanford-ner.jar')\n",
    "    ner = st.tag(word_tokenize(text))\n",
    "    string = \"\"\n",
    "    for item in ner:\n",
    "        if item[1] == 'O':\n",
    "            if item[0] == '<' or item[0] == '@':\n",
    "                string += item[0]\n",
    "            elif item[0] == '>':\n",
    "                    string = string[:-1] + item[0] + ' '\n",
    "            else:\n",
    "                string += item[0] + ' '\n",
    "        else:\n",
    "            string += item[1] + ' '\n",
    "    tweet = ''\n",
    "    for word in string.split():\n",
    "        if word.isupper():\n",
    "            tweet += word + ' '\n",
    "        else:\n",
    "            tweet += word.lower() + ' '\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123.62358642972536"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "np.mean(data.loc[data['label']==1,'text'].apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103.26382978723404"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data.loc[data['label']==-1,'text'].apply(len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
